# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDbwSx2fqWJVL3e-NICxHogUS75aSbQU
"""

!pip install faiss-cpu sentence-transformers torch tqdm --quiet

# main.py
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
import faiss
import pickle
import numpy as np
import re
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder

# ===============================
# --- CONFIGURATION ---
# ===============================
PUBMED_INDEX_PATH = "./pubmed_index.faiss"
PUBMED_META_PATH = "./pubmed_meta.pkl"
MED_INDEX_PATH = "./medquad_index.faiss"
MED_META_PATH = "./medquad_meta.pkl"

device = "cuda" if torch.cuda.is_available() else "cpu"

# ===============================
# --- LOAD MODELS ---
# ===============================
print(f"âš¡ Device: {device}")

EMBED_MODEL_ID = "pritamdeka/S-BioBert-snli-multinli-stsb"
embed_model = SentenceTransformer(EMBED_MODEL_ID, device=device)
embed_dim = embed_model.get_sentence_embedding_dimension()
print(f"âœ… Embedding model loaded: {EMBED_MODEL_ID}, dims: {embed_dim}")

RERANKER_ID = "cross-encoder/ms-marco-MiniLM-L-6-v2"
try:
    reranker = CrossEncoder(RERANKER_ID, device=device)
    print("âœ… Cross-encoder loaded:", RERANKER_ID)
except Exception as e:
    reranker = None
    print("âš  Cross-encoder NOT available; fallback to embedding-only ranking. Error:", e)

# ===============================
# --- LOAD FAISS INDICES ---
# ===============================
print("ðŸ“¦ Loading FAISS indices...")
pub_index = faiss.read_index(PUBMED_INDEX_PATH)
with open(PUBMED_META_PATH, "rb") as f:
    pub_data = pickle.load(f)
    pub_texts = pub_data.get("texts", [])
    pub_meta = pub_data.get("meta", [])

med_index = faiss.read_index(MED_INDEX_PATH)
with open(MED_META_PATH, "rb") as f:
    med_data = pickle.load(f)
    med_texts = med_data.get("texts", [])
    med_meta = med_data.get("meta", [])

print(f"âœ… Loaded indices. Pub chunks: {len(pub_texts)}, Med chunks: {len(med_texts)}")

# ===============================
# --- HELPER FUNCTIONS ---
# ===============================
_SENT_SPLIT = re.compile(r'(?<=[.!?])\s+')

def sentence_split(text):
    if not text:
        return []
    sents = _SENT_SPLIT.split(text.strip())
    return [s.strip() for s in sents if s.strip()]

def short_snippet_from_sentences(sents, max_sents=2):
    return " ".join(sents[:max_sents]).strip()

def extract_best_sentences_for_query(query, passage, top_n_sentences=2, prefer_keywords=True):
    if not passage:
        return ""
    sents = sentence_split(passage)
    if not sents:
        return ""
    sents = sents[:60]
    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")[0]
    sent_embs = embed_model.encode(sents, convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    sims = np.dot(sent_embs, q_emb)

    KEYWORD_GROUPS = {
        "symptoms": ["symptom", "symptoms", "signs", "present", "presenting"],
        "treatment": ["treatment", "treat", "therapy", "manage", "insulin", "medication", "drug", "surgery"],
        "diagnosis": ["diagnos", "test", "screen", "imaging", "ct", "mri", "biopsy"],
        "biomarker": ["biomarker", "marker", "csf", "blood", "plasma", "tau", "amyloid"]
    }

    q_lower = query.lower()
    query_keywords = set()
    for klist in KEYWORD_GROUPS.values():
        for k in klist:
            if k in q_lower:
                query_keywords.update(klist)
                break

    boosts = np.zeros_like(sims)
    if prefer_keywords and query_keywords:
        for i, sent in enumerate(sents):
            if any(k in sent.lower() for k in query_keywords):
                boosts[i] += 0.12

    final_scores = sims + boosts
    top_idx = np.argsort(final_scores)[::-1][:top_n_sentences]
    chosen = [sents[i] for i in sorted(top_idx)]
    return short_snippet_from_sentences(chosen, max_sents=top_n_sentences)

def pick_best_candidate(query, candidates, use_reranker=True):
    if not candidates:
        return None, None
    if reranker and use_reranker:
        try:
            pairs = [(query, c["text"]) for c in candidates]
            scores = reranker.predict(pairs)
            for c, s in zip(candidates, scores):
                c["rerank_score"] = float(s)
            best = max(candidates, key=lambda x: x["rerank_score"])
            return best, float(best["rerank_score"])
        except Exception as e:
            print("âš  Reranker failed; fallback to similarity:", e)
    best = max(candidates, key=lambda x: x["sim"])
    return best, best["sim"]

SYM_KEYS = ["symptom", "symptoms", "signs", "presentation"]
TREAT_KEYS = ["treatment", "treat", "therapy", "manage", "medication", "insulin", "drug", "surgery"]
DIAG_KEYS = ["diagnos", "diagnostic", "test", "screening", "imaging", "ct", "mri", "biopsy"]
RESEARCH_HINTS = ["mechanism", "pathway", "expression", "elevat", "increase", "decrease", "association", "biomarker", "levels"]

def detect_question_type(q):
    ql = q.lower()
    if any(k in ql for k in SYM_KEYS + TREAT_KEYS + DIAG_KEYS):
        return "consumer"
    if any(k in ql for k in RESEARCH_HINTS) and ("?" in q or "are " in ql or "do " in ql):
        return "research"
    if ql.strip().startswith(("what", "how", "when", "why", "who")):
        return "consumer"
    return "consumer"

def unified_precise_answer(query, med_topk=10, pub_topk=10, rerank_topk=12, sentence_count=2, threshold=0.45):
    q_type = detect_question_type(query)
    if q_type == "consumer":
        med_k, pub_k = med_topk, max(1, pub_topk // 3)
    else:
        pub_k, med_k = pub_topk, max(1, med_topk // 3)

    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    med_sims, med_idxs = med_index.search(q_emb, med_k)
    pub_sims, pub_idxs = pub_index.search(q_emb, pub_k)

    candidates = []
    for i, idx in enumerate(med_idxs[0]):
        if idx >= 0:
            sim = float(med_sims[0][i])
            meta = med_meta[idx] if isinstance(med_meta[idx], dict) else {}
            text = meta.get("answer") or med_texts[idx]
            candidates.append({"source": "MedQuAD", "text": text, "sim": sim, "meta": meta})
    for i, idx in enumerate(pub_idxs[0]):
        if idx >= 0:
            sim = float(pub_sims[0][i])
            meta = pub_meta[idx] if isinstance(pub_meta[idx], dict) else {}
            text = pub_texts[idx]
            candidates.append({"source": "PubMedQ", "text": text, "sim": sim, "meta": meta})

    if not candidates:
        return None

    unique = {}
    for c in candidates:
        t = c["text"].strip()
        if t and (t not in unique or c["sim"] > unique[t]["sim"]):
            unique[t] = c
    candidates = sorted(unique.values(), key=lambda x: x["sim"], reverse=True)[:rerank_topk]
    best_cand, final_score = pick_best_candidate(query, candidates)

    if not best_cand:
        return None

    best_snippet = extract_best_sentences_for_query(query, best_cand["text"], top_n_sentences=sentence_count)
    if not best_snippet:
        best_snippet = short_snippet_from_sentences(sentence_split(best_cand["text"]), sentence_count)

    return {"answer": best_snippet}

# ===============================
# --- FASTAPI SETUP ---
# ===============================
app = FastAPI(title="Biomedical QA API", version="1.0")

# Enable CORS for everyone
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # allow any domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def home():
    return {"message": "Welcome to the Biomedical QA API. Use /ask?question=your_query_here"}

@app.get("/health")
def health():
    return {"status": "ok"}

@app.get("/ask")
def ask_question(question: str = Query(..., description="Biomedical question")):
    try:
        result = unified_precise_answer(question)
        if result and result.get("answer"):
            return result
        return {"answer": "No relevant answer found."}
    except Exception as e:
        return {"answer": f"Error: {str(e)}"}